# 2. 비용함수를 어떻게 최소화 하는가?

> (Linear Regression and How to minimize cost)

마찬가지로 개념 공부를 coursera- Andrew Ng Machine Learning 강의를 통해 했기에 내용을 생략하고 이후 시간이 되면 이곳에도 정리해보는 것으로 한다.

바로 텐서플로를 이용한 코드 구현 단계로 넘어간다.

## Pure Python 코드로 구현한 Cost Function

(참고로 밑에 수식은 노션\mathb를 쓰면 되는데 기호들을 정확히 쓰려면 [웹검색](https://www.notion.so/14-1f5fa00916414a43b4c7d77ee7dcf9ed)을 해야한다.. 조금은 불편..ㅎ)

$$cost(W) = \frac{1}{m}\sum_{i=1}^{m}(Wx_i-y_i)^2$$

```python
import numpy as np

X=np.array([1,2,3])
Y=np.array([1,2,3])

def cost_func(W,X,Y):
	c =0
	for i in range(len(x)):
		c += (W*X[i] - Y[i])**2 #편차제곱을 누적해서
	return c/len(x) #평균을 낸 것

for feed_W in np.linspace(-3,5,num=15):
	curr_cost = cost_func(feed_w, X, Y)
	print("{:6.3f}|{:10.5f}".format(feed_W, curr_cost))
```

이제 텐서플로우 코드로 옮겨보면( 아래의 업데이트 되는 W값에 대한 코드임)

![2%20%E1%84%87%E1%85%B5%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%E1%84%85%E1%85%B3%E1%86%AF%20%E1%84%8B%E1%85%A5%E1%84%84%E1%85%A5%E1%87%82%E1%84%80%E1%85%A6%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A9%E1%84%92%E1%85%AA%20%E1%84%92%E1%85%A1%E1%84%82%E1%85%B3%E1%86%AB%E1%84%80%E1%85%A1%206b8345ce3ee644e196c3bae0ccbe4b84/Untitled.png](2%20%E1%84%87%E1%85%B5%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%E1%84%85%E1%85%B3%E1%86%AF%20%E1%84%8B%E1%85%A5%E1%84%84%E1%85%A5%E1%87%82%E1%84%80%E1%85%A6%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A9%E1%84%92%E1%85%AA%20%E1%84%92%E1%85%A1%E1%84%82%E1%85%B3%E1%86%AB%E1%84%80%E1%85%A1%206b8345ce3ee644e196c3bae0ccbe4b84/Untitled.png)

```python
alpha = 0.01
gradient = tf.reduce_mean(tf.multiply(tf.multiply(W,X)-(Y,X))
#descent는 새로운 W값
descent = W - tf.multiply(alpha, gradient)
W.assign(descent)

```

Gradient descent 를 다시 구현해서 learning 시켜보면

```python
#random_seed 초기화시키기 (이게 뭐임?)
tf.set_random_seed(0) # for reproducibility

x_data = [1.,2.,3.,4.]
y_data = [1.,3.,4.,5.]

W = tf.Variable(tf.random_normal([1],-100.,100.))

for step in range(300):
	hypothesis = W*X
	cost = tf.reduce_mean(tf. square(hypothesis - y))

	alpha = 0.01
	gradient = tf.reduce_mean(tf.multiply(tf.multiply(W,X)-(Y,X))
	descent = W - tf.multiply(alpha, gradient)
	W.assign(descent)

	if step %10 ==0:
		print('{:5}|{:10.4f}|{:10.6f}'.format(step, cost.numpy(), W.numpy()[0]))
```